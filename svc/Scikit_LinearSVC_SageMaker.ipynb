{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37432bitd21360393e8941c589655d0ad1eeb8e5",
   "display_name": "Python 3.7.4 32-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop, Train, Optimize, and Deploy Scikit-Learn LinearSVC\n",
    "## Twitter Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Using bucket sagemaker-us-east-1-159307201141\n"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "# SageMaker Python SDK\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "print('Using bucket ' + bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = ['target', 'ids', 'date', 'flag', 'user', 'tweet']\n",
    "DATASET_ENCODING = 'ISO-8859-1'\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Read the data locally\n",
    "df = pd.read_csv('svc/training.1600000.processed.noemoticon.csv',\n",
    "                 encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "\n",
    "# Split the data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['tweet'], df['target'], test_size=1 - TRAIN_SIZE, random_state=817)\n",
    "\n",
    "trainX = pd.DataFrame(X_train, columns=['tweet'])\n",
    "trainX['target'] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test, columns=['tweet'])\n",
    "testX['target'] = y_test\n",
    "\n",
    "# Save the train_test_split locally\n",
    "trainX.to_csv('svc/twitter_train.csv', index=False)\n",
    "testX.to_csv('svc/twitter_test.csv', index=False)\n",
    "\n",
    "# Send data to S3. SageMaker will take training data from S3\n",
    "trainpath = sess.upload_data(\n",
    "    path='svc/twitter_train.csv', bucket=bucket,\n",
    "    key_prefix='data/twitter')\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path='svc/twitter_test.csv', bucket=bucket,\n",
    "    key_prefix='data/twitter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing a Script Mode Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile script.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from sagemaker_containers.beta.framework import worker\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "TWEET_CLEANING_RE = r'@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+'\n",
    "\n",
    "decode_map = {\n",
    "    0: 'NEGATIVE',\n",
    "    2: 'NEUTRAL',\n",
    "    4: 'POSITIVE'\n",
    "}\n",
    "\n",
    "\n",
    "# TODO Do I need this?\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]\n",
    "\n",
    "\n",
    "# TODO Adjust preprocessor to be better\n",
    "def preprocess(tweet, stem=False):\n",
    "    \"\"\"Preprocesses one tweet\"\"\"\n",
    "    tweet = re.sub(TWEET_CLEANING_RE, ' ', str(tweet).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in tweet.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, 'model.joblib'))\n",
    "    return clf\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"An input_fun that loads JSON into a Pandas DataFrame, and preprocesses the tweets\"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        # TODO We don't really need Pandas here anymore\n",
    "        df = pd.read_json(StringIO(request_body))\n",
    "        # TODO Add way to provide stem parameter\n",
    "        df.tweet = df.tweet.apply(lambda x: preprocess(x))\n",
    "        return df['tweet'].to_numpy()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            '{} is not supported by script.'.format(request_content_type))\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    pred = model.predict(input_data)\n",
    "    deci_func = model.decision_function(input_data)\n",
    "\n",
    "    predictions = []\n",
    "    for p, d in zip(pred, deci_func):\n",
    "        predictions.append({\n",
    "            'prediction': p,\n",
    "            'probability': d\n",
    "        })\n",
    "\n",
    "    return {'results': predictions}\n",
    "\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    if content_type == 'application/json':\n",
    "        return worker.Response(json.dumps(prediction), content_type, mimetype=content_type)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            '{} accept type is not supported by this script.'.format(content_type))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Extracting arguments')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters from the client\n",
    "    parser.add_argument('--stem', type=bool, default=False)\n",
    "    parser.add_argument('--use-idf', type=str, default='true')\n",
    "    parser.add_argument('--smooth-idf', type=str, default='true')\n",
    "    parser.add_argument('--sublinear-tf', type=str, default='true')\n",
    "    parser.add_argument('--C', type=float, default=1.0)\n",
    "    parser.add_argument('--penalty', type=str, default='l2')\n",
    "    parser.add_argument('--loss', type=str, default='squared_hinge')\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    # TODO Remove this argument?\n",
    "    # parser.add_argument('--output-data-dir', type=str,\n",
    "    #                     default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model-dir', type=str,\n",
    "                        default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str,\n",
    "                        default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str,\n",
    "                        default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='twitter_train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='twitter_test.csv')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print('Reading Tweets')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    print('Length of train_df: {}'.format(str(len(train_df.index))))\n",
    "    print('Length of test_df: {}'.format(str(len(test_df.index))))\n",
    "\n",
    "    print('Preprocessing the Tweets')\n",
    "    # Decode Sentiment/Target\n",
    "    train_df.target = train_df.target.apply(lambda x: decode_sentiment(x))\n",
    "    test_df.target = test_df.target.apply(lambda x: decode_sentiment(x))\n",
    "\n",
    "    # Preprocess Tweet\n",
    "    train_df.tweet = train_df.tweet.apply(lambda x: preprocess(x, args.stem))\n",
    "    test_df.tweet = test_df.tweet.apply(lambda x: preprocess(x, args.stem))\n",
    "\n",
    "    print('Building training and testing datasets')\n",
    "    X_train = train_df['tweet']\n",
    "    y_train = train_df['target']\n",
    "    X_test = test_df['tweet']\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    # TODO TfIdf preprocessor?\n",
    "\n",
    "    print('Training the model')\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(use_idf=bool(strtobool(args.use_idf)),\n",
    "                                  smooth_idf=bool(strtobool(args.smooth_idf)),\n",
    "                                  sublinear_tf=bool(strtobool(args.sublinear_tf)))),\n",
    "        ('svc', LinearSVC(C=args.C,\n",
    "                          penalty=args.penalty,\n",
    "                          loss=args.loss))\n",
    "    ])\n",
    "\n",
    "    clf = pipe.fit(X_train, y_train)\n",
    "\n",
    "    print('Print validation statistics')\n",
    "    pred = clf.predict(X_test)\n",
    "    # pred_prob = clf.predict_proba(X_test)\n",
    "    decision = clf.decision_function(X_test)\n",
    "\n",
    "    print(pred)\n",
    "    print(decision)\n",
    "\n",
    "    # TODO Why the fuck are these the same?\n",
    "    print('Accuracy: {}'.format(accuracy_score(y_test, pred)))\n",
    "    # print('Precision: {}'.format(precision_score(y_test, pred, average='macro')))\n",
    "    # print('Recall: {}'.format(recall_score(y_test, pred, average='micro')))\n",
    "    # TODO Add more validation statistics\n",
    "\n",
    "    print('Save the model')\n",
    "    joblib.dump(clf, os.path.join(args.model_dir, 'model.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python svc/script.py -C 6.8437521959309535 \\\n",
    "                    --smooth_idf false \\\n",
    "                    --sublinear_tf false \\\n",
    "                    -- use_idf true \\\n",
    "                    --model-dir svc/ \\\n",
    "                    --train svc/ \\\n",
    "                    --test svc/"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SageMaker Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='svc.py',\n",
    "    source_dir='svc/',\n",
    "    role = get_execution_role(),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    framework_version='0.20.0',\n",
    "    base_job_name='twitter-svc',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'accuracy',\n",
    "         'Regex': 'Accuracy: ([0-9.]+).*$'}],\n",
    "    hyperparameters={\n",
    "        'C': 6.8437521959309535,\n",
    "        'smooth_idf': 'false',\n",
    "        'sublinear_tf': 'true',\n",
    "        'use_idf': 'true'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator.fit({'train':trainpath, 'test':testpath})"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Launching a tuning job with the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Hyperparameter Tuner \n",
    "\n",
    "from sagemaker.tuner import ContinuousParameter, CategoricalParameter\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'use_idf': CategoricalParameter(['true', 'false']),\n",
    "    'smooth_idf': CategoricalParameter(['true', 'false']),\n",
    "    'sublinear_tf': CategoricalParameter(['true', 'false']),\n",
    "    'C': ContinuousParameter(0.0001, 100, 'Logarithmic')\n",
    "    # 'penalty': CategoricalParameter(['l1', 'l2']),\n",
    "    # 'loss': CategoricalParameter(['hinge', 'squared_hinge'])\n",
    "}\n",
    "\n",
    "Optimizer = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=sklearn_estimator,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    base_tuning_job_name='twitter-svc-tuner',\n",
    "    objective_type='Maximize',\n",
    "    objective_metric_name='accuracy',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'accuracy',\n",
    "         'Regex': 'Accuracy: ([0-9.]+).*$'}\n",
    "    ],\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer.fit({'train':trainpath, 'test':testpath})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = Optimizer.analytics().dataframe()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deploy to a real-time endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2020-03-10 01:47:30 Starting - Preparing the instances for training\n2020-03-10 01:47:30 Downloading - Downloading input data\n2020-03-10 01:47:30 Training - Training image download completed. Training in progress.\n2020-03-10 01:47:30 Uploading - Uploading generated training model\n2020-03-10 01:47:30 Completed - Training job completed\u001b[34m2020-03-10 01:46:40,466 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n\u001b[34m2020-03-10 01:46:40,468 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n\u001b[34m2020-03-10 01:46:40,478 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n\u001b[34m2020-03-10 01:46:57,241 sagemaker-containers INFO     Module svc does not provide a setup.py. \u001b[0m\n\u001b[34mGenerating setup.py\u001b[0m\n\u001b[34m2020-03-10 01:46:57,241 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n\u001b[34m2020-03-10 01:46:57,242 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n\u001b[34m2020-03-10 01:46:57,242 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n\u001b[34m/miniconda3/bin/python -m pip install . -r requirements.txt\u001b[0m\n\u001b[34mProcessing /opt/ml/code\u001b[0m\n\u001b[34mCollecting nltk\n  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\u001b[0m\n\u001b[34mRequirement already satisfied: six in /miniconda3/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n\u001b[34mBuilding wheels for collected packages: nltk, svc\n  Building wheel for nltk (setup.py): started\n  Building wheel for nltk (setup.py): finished with status 'done'\n  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449910 sha256=1b13b3f1d33839fef0e7cc1ac4c7941033b165ab34abc4db21e8c9915592ee66\n  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n  Building wheel for svc (setup.py): started\u001b[0m\n\u001b[34m  Building wheel for svc (setup.py): finished with status 'done'\u001b[0m\n\u001b[34m  Created wheel for svc: filename=svc-1.0.0-py2.py3-none-any.whl size=94179054 sha256=293eda105f0bcdeb9a53727389872db28031c80aa9d04b580eb0084f679b362e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-x7kim_4y/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n\u001b[34mSuccessfully built nltk svc\u001b[0m\n\u001b[34mInstalling collected packages: nltk, svc\u001b[0m\n\u001b[34mSuccessfully installed nltk-3.4.5 svc-1.0.0\u001b[0m\n\u001b[34m2020-03-10 01:47:18,685 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n\u001b[34m2020-03-10 01:47:18,695 sagemaker-containers INFO     Invoking user script\n\u001b[0m\n\u001b[34mTraining Env:\n\u001b[0m\n\u001b[34m{\n    \"additional_framework_parameters\": {},\n    \"channel_input_dirs\": {\n        \"test\": \"/opt/ml/input/data/test\",\n        \"train\": \"/opt/ml/input/data/train\"\n    },\n    \"current_host\": \"algo-1\",\n    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n    \"hosts\": [\n        \"algo-1\"\n    ],\n    \"hyperparameters\": {\n        \"sublinear_tf\": \"true\",\n        \"use_idf\": \"true\",\n        \"C\": 6.8437521959309535,\n        \"smooth_idf\": \"false\"\n    },\n    \"input_config_dir\": \"/opt/ml/input/config\",\n    \"input_data_config\": {\n        \"test\": {\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        },\n        \"train\": {\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        }\n    },\n    \"input_dir\": \"/opt/ml/input\",\n    \"is_master\": true,\n    \"job_name\": \"twitter-svc-2020-03-10-01-42-11-418\",\n    \"log_level\": 20,\n    \"master_hostname\": \"algo-1\",\n    \"model_dir\": \"/opt/ml/model\",\n    \"module_dir\": \"s3://sagemaker-us-east-1-159307201141/twitter-svc-2020-03-10-01-42-11-418/source/sourcedir.tar.gz\",\n    \"module_name\": \"svc\",\n    \"network_interface_name\": \"eth0\",\n    \"num_cpus\": 2,\n    \"num_gpus\": 0,\n    \"output_data_dir\": \"/opt/ml/output/data\",\n    \"output_dir\": \"/opt/ml/output\",\n    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n    \"resource_config\": {\n        \"current_host\": \"algo-1\",\n        \"hosts\": [\n            \"algo-1\"\n        ],\n        \"network_interface_name\": \"eth0\"\n    },\n    \"user_entry_point\": \"svc.py\"\u001b[0m\n\u001b[34m}\n\u001b[0m\n\u001b[34mEnvironment variables:\n\u001b[0m\n\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n\u001b[34mSM_HPS={\"C\":6.8437521959309535,\"smooth_idf\":\"false\",\"sublinear_tf\":\"true\",\"use_idf\":\"true\"}\u001b[0m\n\u001b[34mSM_USER_ENTRY_POINT=svc.py\u001b[0m\n\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n\u001b[34mSM_MODULE_NAME=svc\u001b[0m\n\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n\u001b[34mSM_NUM_CPUS=2\u001b[0m\n\u001b[34mSM_NUM_GPUS=0\u001b[0m\n\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-159307201141/twitter-svc-2020-03-10-01-42-11-418/source/sourcedir.tar.gz\u001b[0m\n\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"C\":6.8437521959309535,\"smooth_idf\":\"false\",\"sublinear_tf\":\"true\",\"use_idf\":\"true\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"twitter-svc-2020-03-10-01-42-11-418\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-159307201141/twitter-svc-2020-03-10-01-42-11-418/source/sourcedir.tar.gz\",\"module_name\":\"svc\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"svc.py\"}\u001b[0m\n\u001b[34mSM_USER_ARGS=[\"-C\",\"6.8437521959309535\",\"--smooth_idf\",\"false\",\"--sublinear_tf\",\"true\",\"--use_idf\",\"true\"]\u001b[0m\n\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n\u001b[34mSM_HP_SUBLINEAR_TF=true\u001b[0m\n\u001b[34mSM_HP_USE_IDF=true\u001b[0m\n\u001b[34mSM_HP_C=6.8437521959309535\u001b[0m\n\u001b[34mSM_HP_SMOOTH_IDF=false\u001b[0m\n\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n\u001b[0m\n\u001b[34mInvoking script with the following command:\n\u001b[0m\n\u001b[34m/miniconda3/bin/python -m svc -C 6.8437521959309535 --smooth_idf false --sublinear_tf true --use_idf true\n\n\u001b[0m\n\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  import imp\u001b[0m\n\u001b[34m[nltk_data] Downloading package stopwords to /root/nltk_data...\u001b[0m\n\u001b[34m[nltk_data]   Unzipping corpora/stopwords.zip.\u001b[0m\n\u001b[34mExtracting arguments\u001b[0m\n\u001b[34mReading Tweets\u001b[0m\n\u001b[34mLength of train_df: 1280\u001b[0m\n\u001b[34mLength of test_df: 320\u001b[0m\n\u001b[34mPreprocessing the Tweets\u001b[0m\n\u001b[34mBuilding training and testing datasets\u001b[0m\n\u001b[34mTraining the model\u001b[0m\n\u001b[34mPrint validation statistics\u001b[0m\n\u001b[34m['NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE'\n 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE'\n 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE'\n 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE'\n 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE'\n 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE'\n 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE'\n 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE'\n 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE'\n 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE'\n 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE'\n 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE'\n 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE'\n 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE'\n 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE'\n 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE'\n 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE'\n 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE'\n 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE'\n 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE'\n 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE'\n 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE'\n 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE'\n 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE'\n 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE'\n 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE'\n 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'POSITIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE'\n 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE'\n 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE'\n 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE'\n 'POSITIVE' 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE'\n 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE' 'NEGATIVE' 'POSITIVE'\n 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE' 'POSITIVE'\n 'NEGATIVE' 'POSITIVE' 'NEGATIVE' 'NEGATIVE' 'NEGATIVE' 'POSITIVE'\n 'POSITIVE' 'NEGATIVE']\u001b[0m\n\u001b[34m[-7.80680377e-01 -4.34476643e-01 -5.94934920e-01  1.34148630e+00\n -5.64021200e-01  1.03298568e+00 -6.03989962e-01  3.59375903e-01\n -6.83263530e-01  4.97100653e-01  4.50308751e-01 -3.24124060e-02\n  7.66347144e-01  2.86086156e-02 -4.28965118e-02  8.03167816e-01\n  4.50742125e-01  2.26560007e-01 -3.50358553e-01 -2.83918541e-02\n  7.02002765e-01  2.60970776e-01  7.40177383e-01  7.44757761e-02\n  6.26878278e-02 -6.52997320e-01 -3.44863320e-01  2.08165857e-02\n -8.13445360e-01  2.80493223e-01 -1.22885236e+00  3.53686961e-01\n  1.58547140e-01 -6.71323845e-02  2.31363004e-01 -9.05109111e-01\n  2.25488686e-01 -4.32276378e-01  1.71478263e-01  1.24284522e-01\n -2.35961002e-01 -3.25575443e-02  3.45350306e-01  7.17231752e-01\n -2.65556614e-01 -1.32146008e+00  2.12453639e-01  1.45017071e-01\n  9.96574046e-01  4.25982270e-01  4.30040487e-01 -3.30503180e-01\n -2.81411989e-01  2.80077780e-02  3.72110542e-01  5.18692112e-01\n -8.26031040e-01  1.69252033e-02  6.62554080e-01 -5.07434516e-01\n  4.64644784e-01  2.74969217e-01  8.70164190e-01 -4.29568686e-01\n  3.35390470e-01  5.23886941e-01 -7.44522819e-03  6.50917754e-01\n  2.31620886e-01  9.52019194e-01 -3.40175295e-01  7.53695869e-01\n  8.88757461e-02  1.51570691e-01 -7.27201006e-01 -2.27154922e-01\n  1.10689784e-01 -5.44090177e-01 -4.85901126e-01  3.33794171e-01\n -1.98902722e-01  6.77154303e-02  2.08736478e-01 -2.97931056e-01\n  9.82600429e-01  5.00177415e-01  8.78144373e-01 -1.42975399e+00\n -2.40220872e-01  1.25302466e-01  1.07839880e+00  3.49002803e-01\n  5.55672477e-01 -8.90727074e-01 -3.38245707e-01  1.60140593e+00\n  5.61545195e-01  6.61327530e-01  5.77988921e-01 -9.73667324e-01\n  8.22870257e-01 -6.96049892e-01 -2.18536435e-01  1.69252033e-02\n  3.12413082e-02  6.46955695e-01  3.54280534e-01  4.65669856e-01\n  2.57483753e-01  5.55206676e-01 -3.59012038e-01 -1.95470268e-01\n -2.29926618e-01  5.67584050e-02  4.21134283e-01  9.34745713e-01\n  7.30487015e-01 -3.72714787e-01  4.91476378e-01 -1.09196984e-02\n  2.54712194e-01  2.70548871e-03 -7.51883582e-01 -9.54468376e-01\n  5.61271964e-01 -4.11623843e-01  3.70905654e-01 -1.29171006e-01\n  1.69252033e-02 -9.31483412e-01 -1.62365930e-01 -4.66946015e-01\n -1.36271801e-01 -1.02288149e-01  1.32418722e-01 -3.94531612e-01\n  3.51022245e-01  5.16878781e-01 -5.14894011e-01 -3.89050269e-01\n -5.81720021e-01  1.16957502e+00  3.21721668e-01 -8.07283502e-01\n -1.12431902e+00 -9.37132563e-02  1.34461152e+00  3.99661770e-02\n  7.89717380e-01 -3.88134935e-02  4.25105476e-01  3.95737663e-01\n  1.20377816e-01  5.30153384e-01 -1.66411170e-01 -3.22323903e-01\n -1.53201347e+00 -8.26933387e-01  3.19604274e-02  3.93270854e-01\n -7.58768128e-01 -8.56441102e-02 -6.09650330e-02 -8.89533632e-01\n -3.72070167e-01 -7.05383702e-01 -9.03973729e-01 -1.40500966e+00\n -4.22199214e-01 -3.02289180e-01 -4.09409592e-01  2.93811011e-01\n  6.25752586e-01  3.72512813e-01 -4.55941727e-01  5.75148811e-01\n  3.71781263e-03  7.53978645e-02 -1.05272280e-03  2.84161771e-01\n  1.69252033e-02 -5.53863939e-01  3.30266650e-01 -4.39380234e-01\n  7.43882494e-01 -2.24804830e-01  6.62648541e-01  5.64803855e-01\n  1.18596716e-01  8.94222144e-01  2.85041596e-01 -7.33469814e-01\n  6.03449064e-01 -2.67969162e-01  2.90263615e-01  4.88379913e-01\n -4.68248040e-01  2.14965067e-01  7.68722474e-01  6.52877265e-01\n -5.17871345e-01  8.43502908e-02 -5.46220674e-01 -1.06328597e+00\n  4.10781000e-01  8.29514839e-01 -6.16398045e-01 -6.85520972e-02\n -1.29534246e+00 -3.31373978e-01 -4.80198385e-02 -3.12701290e-01\n  8.32560473e-01  6.89142590e-01 -5.90833500e-02  9.91093055e-01\n -9.18483488e-01  1.69252033e-02 -1.60878494e-01 -8.82415731e-01\n -9.16654527e-02 -2.60519121e-01  7.39147616e-01 -5.80682684e-02\n  3.57700750e-01  1.16314530e+00 -7.76827348e-01 -9.55986846e-01\n  6.41382066e-01 -3.34174286e-01  4.87244837e-01  4.25982270e-01\n -3.05837289e-01 -2.01974412e-01 -5.45768626e-02 -7.52435180e-01\n -3.25735349e-01  1.71105939e-01  4.25894645e-01  1.08482424e+00\n  1.28799134e-01  1.99586568e-01  3.44683957e-01 -1.12908929e-01\n  5.96545403e-02 -3.84409532e-01  1.69252033e-02  9.11327291e-01\n -4.59330757e-01 -7.64563780e-02 -3.67188688e-02 -2.62600825e-01\n  5.39908216e-01  2.71199252e-01  1.69252033e-02  1.20865166e-01\n  5.87222329e-02  2.55772766e-01 -3.21260806e-01  1.12168746e+00\n -4.16060118e-01  7.32000792e-01 -2.83583860e-02 -5.59163583e-02\n  1.69252033e-02 -6.90558610e-01 -1.40286718e-01  5.09752840e-01\n  5.51558395e-01 -2.19005145e-01  2.53718920e-01  8.37598602e-01\n  5.64558149e-02  1.28555673e-01 -2.81641675e-01  6.08716587e-01\n -8.81885250e-01  1.26005283e-01  8.22307038e-01 -4.32232120e-01\n -8.64262747e-01 -6.53161925e-01 -6.88246096e-01  3.86675717e-01\n -9.31898723e-01 -1.96675688e-01  2.03157101e-01  3.41717292e-02\n -3.90615884e-01  4.48513467e-01 -4.88087996e-01  3.29057381e-01\n  8.03435679e-01 -5.03577173e-01  9.40718745e-02 -3.52900632e-01\n  1.43902702e-01 -3.55329275e-01 -7.30614961e-01  1.95503152e-01\n -4.14129202e-01 -1.10605594e-01  2.17033234e-01  5.34984006e-01\n -1.39036641e-01  6.61327530e-01 -2.49945124e-01 -9.30354346e-01\n -1.16495546e-01 -2.47974883e-01  4.99338506e-01  2.24292522e-01\n -8.90366913e-01  9.44814133e-01 -4.85886523e-01 -5.04965971e-01\n -1.15698781e+00  2.42335643e-01  6.58411887e-01 -1.58273041e-01]\u001b[0m\n\u001b[34mAccuracy: 0.646875\u001b[0m\n\u001b[34mSave the model\u001b[0m\n\u001b[34m2020-03-10 01:47:21,687 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\nTraining seconds: 103\nBillable seconds: 103\n------------------!"
    }
   ],
   "source": [
    "#TODO Get best model from a tuning job OR do it by name\n",
    "\n",
    "my_training_job_name = \"twitter-svc-2020-03-10-01-42-11-418\"\n",
    "sklearn_estimator = SKLearn.attach(my_training_job_name)\n",
    "\n",
    "predictor = sklearn_estimator.deploy(\n",
    "    instance_type='ml.t2.medium',\n",
    "    initial_instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Invoke with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "b'{\"results\": [{\"prediction\": \"POSITIVE\", \"probability\": 1.2966381957664954}, {\"prediction\": \"POSITIVE\", \"probability\": 0.7260198933212587}, {\"prediction\": \"NEGATIVE\", \"probability\": -0.1334945546460503}, {\"prediction\": \"POSITIVE\", \"probability\": 0.8456000844949692}, {\"prediction\": \"NEGATIVE\", \"probability\": -0.34600364036305226}]}'\n"
    }
   ],
   "source": [
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "tweets = {\n",
    "    'tweet': [\n",
    "        '''totally okay to not love Biden but if you’re threatening to just not vote in the election if he’s the candidate, you suck''',\n",
    "        '''Here to give you dinner inspo, so you don't keep eating chips for dinner.''',\n",
    "        '''Fresh pastry from Beetbox in the house!''',\n",
    "        '''Spent 2 days in New Orleans. Gained 75 lbs. Totally worth it.''',\n",
    "        '''These people made me laugh so hard that I briefly thought I gave myself a hernia.'''\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint,\n",
    "    Body=json.dumps(tweets),\n",
    "    ContentType='application/json')\n",
    "\n",
    "print(response['Body'].read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3.delete_endpoint(EndpointName=predictor.endpoint)"
   ]
  }
 ]
}