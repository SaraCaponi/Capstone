{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37432bitd21360393e8941c589655d0ad1eeb8e5",
   "display_name": "Python 3.7.4 32-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop, Train, Optimize, and Deploy Scikit-Learn LinearSVC\n",
    "## Twitter Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SageMaker Python SDK\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "print('Using bucket ' + bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = ['target', 'ids', 'date', 'flag', 'user', 'tweet']\n",
    "DATASET_ENCODING = 'ISO-8859-1'\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Read the data locally\n",
    "df = pd.read_csv('svc/training.1600000.processed.noemoticon.csv',\n",
    "                 encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "\n",
    "# Split the data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['tweet'], df['target'], test_size=1 - TRAIN_SIZE, random_state=817)\n",
    "\n",
    "trainX = pd.DataFrame(X_train, columns=['tweet'])\n",
    "trainX['target'] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test, columns=['tweet'])\n",
    "testX['target'] = y_test\n",
    "\n",
    "# Save the train_test_split locally\n",
    "trainX.to_csv('svc/twitter_train.csv', index=False)\n",
    "testX.to_csv('svc/twitter_test.csv', index=False)\n",
    "\n",
    "# Send data to S3. SageMaker will take training data from S3\n",
    "trainpath = sess.upload_data(\n",
    "    path='svc/twitter_train.csv', bucket=bucket,\n",
    "    key_prefix='data/twitter')\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path='svc/twitter_test.csv', bucket=bucket,\n",
    "    key_prefix='data/twitter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing a Script Mode Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile script.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from sagemaker_containers.beta.framework import worker\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "TWEET_CLEANING_RE = r'@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+'\n",
    "\n",
    "decode_map = {\n",
    "    0: 'NEGATIVE',\n",
    "    4: 'POSITIVE'\n",
    "}\n",
    "\n",
    "\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]\n",
    "\n",
    "\n",
    "# TODO Adjust preprocessor to be better\n",
    "def preprocess(tweet, stem=False):\n",
    "    \"\"\"Preprocesses one tweet\"\"\"\n",
    "    tweet = re.sub(TWEET_CLEANING_RE, ' ', str(tweet).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in tweet.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, 'model.joblib'))\n",
    "    return clf\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"An input_fun that loads JSON into a Pandas DataFrame, and preprocesses the tweets\"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        # TODO We don't really need Pandas here anymore\n",
    "        df = pd.read_json(StringIO(request_body))\n",
    "        # TODO Add way to provide stem parameter\n",
    "        df.tweet = df.tweet.apply(lambda x: preprocess(x))\n",
    "        return df['tweet'].to_numpy()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            '{} is not supported by script.'.format(request_content_type))\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    pred = model.predict(input_data)\n",
    "    deci_func = model.decision_function(input_data)\n",
    "\n",
    "    predictions = []\n",
    "    for p, d in zip(pred, deci_func):\n",
    "        predictions.append({\n",
    "            'prediction': p,\n",
    "            'probability': d\n",
    "        })\n",
    "\n",
    "    return {'results': predictions}\n",
    "\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    if content_type == 'application/json':\n",
    "        return worker.Response(json.dumps(prediction), content_type, mimetype=content_type)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            '{} accept type is not supported by this script.'.format(content_type))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Extracting arguments')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters from the client\n",
    "    parser.add_argument('--stem', type=bool, default=False)\n",
    "    parser.add_argument('--use-idf', type=str, default='true')\n",
    "    parser.add_argument('--smooth-idf', type=str, default='true')\n",
    "    parser.add_argument('--sublinear-tf', type=str, default='true')\n",
    "    parser.add_argument('--C', type=float, default=1.0)\n",
    "    parser.add_argument('--penalty', type=str, default='l2')\n",
    "    parser.add_argument('--loss', type=str, default='squared_hinge')\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    # TODO Remove this argument?\n",
    "    # parser.add_argument('--output-data-dir', type=str,\n",
    "    #                     default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model-dir', type=str,\n",
    "                        default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str,\n",
    "                        default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str,\n",
    "                        default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='twitter_train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='twitter_test.csv')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print('Reading Tweets')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    print('Length of train_df: {}'.format(str(len(train_df.index))))\n",
    "    print('Length of test_df: {}'.format(str(len(test_df.index))))\n",
    "\n",
    "    print('Preprocessing the Tweets')\n",
    "    # Decode Sentiment/Target\n",
    "    train_df.target = train_df.target.apply(lambda x: decode_sentiment(x))\n",
    "    test_df.target = test_df.target.apply(lambda x: decode_sentiment(x))\n",
    "\n",
    "    # Preprocess Tweet\n",
    "    train_df.tweet = train_df.tweet.apply(lambda x: preprocess(x, args.stem))\n",
    "    test_df.tweet = test_df.tweet.apply(lambda x: preprocess(x, args.stem))\n",
    "\n",
    "    print('Building training and testing datasets')\n",
    "    X_train = train_df['tweet']\n",
    "    y_train = train_df['target']\n",
    "    X_test = test_df['tweet']\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    # TODO TfIdf preprocessor?\n",
    "\n",
    "    print('Training the model')\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(use_idf=bool(strtobool(args.use_idf)),\n",
    "                                  smooth_idf=bool(strtobool(args.smooth_idf)),\n",
    "                                  sublinear_tf=bool(strtobool(args.sublinear_tf)))),\n",
    "        ('svc', LinearSVC(C=args.C,\n",
    "                          penalty=args.penalty,\n",
    "                          loss=args.loss))\n",
    "    ])\n",
    "\n",
    "    clf = pipe.fit(X_train, y_train)\n",
    "\n",
    "    print('Print validation statistics')\n",
    "    pred = clf.predict(X_test)\n",
    "    # pred_prob = clf.predict_proba(X_test)\n",
    "    decision = clf.decision_function(X_test)\n",
    "\n",
    "    print(pred)\n",
    "    print(decision)\n",
    "\n",
    "    # TODO Why the fuck are these the same?\n",
    "    print('Accuracy: {}'.format(accuracy_score(y_test, pred)))\n",
    "    # print('Precision: {}'.format(precision_score(y_test, pred, average='macro')))\n",
    "    # print('Recall: {}'.format(recall_score(y_test, pred, average='micro')))\n",
    "    # TODO Add more validation statistics\n",
    "\n",
    "    print('Save the model')\n",
    "    joblib.dump(clf, os.path.join(args.model_dir, 'model.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python svc/script.py -C 6.8437521959309535 \\\n",
    "                    --smooth_idf false \\\n",
    "                    --sublinear_tf false \\\n",
    "                    -- use_idf true \\\n",
    "                    --model-dir svc/ \\\n",
    "                    --train svc/ \\\n",
    "                    --test svc/"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SageMaker Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='svc.py',\n",
    "    source_dir='svc/',\n",
    "    role = get_execution_role(),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    framework_version='0.20.0',\n",
    "    base_job_name='twitter-svc',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'accuracy',\n",
    "         'Regex': 'Accuracy: ([0-9.]+).*$'}],\n",
    "    hyperparameters={\n",
    "        'C': 6.8437521959309535,\n",
    "        'smooth_idf': 'false',\n",
    "        'sublinear_tf': 'true',\n",
    "        'use_idf': 'true'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator.fit({'train':trainpath, 'test':testpath})"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Launching a tuning job with the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Hyperparameter Tuner \n",
    "\n",
    "from sagemaker.tuner import ContinuousParameter, CategoricalParameter\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'use_idf': CategoricalParameter(['true', 'false']),\n",
    "    'smooth_idf': CategoricalParameter(['true', 'false']),\n",
    "    'sublinear_tf': CategoricalParameter(['true', 'false']),\n",
    "    'C': ContinuousParameter(0.0001, 100, 'Logarithmic')\n",
    "    # 'penalty': CategoricalParameter(['l1', 'l2']),\n",
    "    # 'loss': CategoricalParameter(['hinge', 'squared_hinge'])\n",
    "}\n",
    "\n",
    "Optimizer = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=sklearn_estimator,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    base_tuning_job_name='twitter-svc-tuner',\n",
    "    objective_type='Maximize',\n",
    "    objective_metric_name='accuracy',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'accuracy',\n",
    "         'Regex': 'Accuracy: ([0-9.]+).*$'}\n",
    "    ],\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer.fit({'train':trainpath, 'test':testpath})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = Optimizer.analytics().dataframe()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deploy to a real-time endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Get best model from a tuning job OR do it by name\n",
    "\n",
    "my_training_job_name = \"twitter-svc-2020-03-10-01-42-11-418\"\n",
    "sklearn_estimator = SKLearn.attach(my_training_job_name)\n",
    "\n",
    "predictor = sklearn_estimator.deploy(\n",
    "    instance_type='ml.c5.large',\n",
    "    initial_instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Invoke with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "tweets = {\n",
    "    'tweet': [\n",
    "        '''totally okay to not love Biden but if you’re threatening to just not vote in the election if he’s the candidate, you suck''',\n",
    "        '''Here to give you dinner inspo, so you don't keep eating chips for dinner.''',\n",
    "        '''Fresh pastry from Beetbox in the house!''',\n",
    "        '''Spent 2 days in New Orleans. Gained 75 lbs. Totally worth it.''',\n",
    "        '''These people made me laugh so hard that I briefly thought I gave myself a hernia.'''\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint,\n",
    "    Body=json.dumps(tweets),\n",
    "    ContentType='application/json')\n",
    "\n",
    "print(response['Body'].read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3.delete_endpoint(EndpointName=predictor.endpoint)"
   ]
  }
 ]
}