{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37432bitd21360393e8941c589655d0ad1eeb8e5",
   "display_name": "Python 3.7.4 32-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop, Train, Optimize, and Deploy Scikit-Learn LinearSVC\n",
    "## Twitter Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using bucket sagemaker-us-east-1-159307201141\n"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "# SageMaker Python SDK\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "print('Using bucket ' + bucket)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = ['target', 'ids', 'date', 'flag', 'user', 'tweet']\n",
    "DATASET_ENCODING = 'ISO-8859-1'\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Read the data locally\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',\n",
    "                 encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "\n",
    "# Split the data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['tweet'], df['target'], test_size=1 - TRAIN_SIZE, random_state=817)\n",
    "\n",
    "trainX = pd.DataFrame(X_train, columns=['tweet'])\n",
    "trainX['target'] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test, columns=['tweet'])\n",
    "testX['target'] = y_test\n",
    "\n",
    "# Save the train_test_split locally\n",
    "trainX.to_csv('twitter_train.csv', index=False)\n",
    "testX.to_csv('twitter_test.csv', index=False)\n",
    "\n",
    "# Send data to S3. SageMaker will take training data from S3\n",
    "trainpath = sess.upload_data(\n",
    "    path='twitter_train.csv', bucket=bucket,\n",
    "    key_prefix='data/twitter')\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path='twitter_test.csv', bucket=bucket,\n",
    "    key_prefix='data/twitter')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing a Script Mode Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overwriting script.py\n"
    }
   ],
   "source": [
    "%%writefile script.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from sagemaker_containers.beta.framework import worker\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "TWEET_CLEANING_RE = r'@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+'\n",
    "\n",
    "decode_map = {\n",
    "    0: 'NEGATIVE',\n",
    "    4: 'POSITIVE'\n",
    "}\n",
    "\n",
    "\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]\n",
    "\n",
    "\n",
    "# TODO Adjust preprocessor to be better\n",
    "def preprocess(tweet, stem=False):\n",
    "    \"\"\"Preprocesses one tweet\"\"\"\n",
    "    tweet = re.sub(TWEET_CLEANING_RE, ' ', str(tweet).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in tweet.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, 'model.joblib'))\n",
    "    return clf\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"An input_fun that loads JSON into a Pandas DataFrame, and preprocesses the tweets\"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        # TODO We don't really need Pandas here anymore\n",
    "        df = pd.read_json(StringIO(request_body))\n",
    "        # TODO Add way to provide stem parameter\n",
    "        df.tweet = df.tweet.apply(lambda x: preprocess(x))\n",
    "        return df['tweet'].to_numpy()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            '{} is not supported by script.'.format(request_content_type))\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    pred = model.predict(input_data)\n",
    "    deci_func = model.decision_function(input_data)\n",
    "\n",
    "    predictions = []\n",
    "    for p, d in zip(pred, deci_func):\n",
    "        predictions.append({\n",
    "            'prediction': p,\n",
    "            'probability': d\n",
    "        })\n",
    "\n",
    "    return {'results': predictions}\n",
    "\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    if content_type == 'application/json':\n",
    "        return worker.Response(json.dumps(prediction), content_type, mimetype=content_type)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            '{} accept type is not supported by this script.'.format(content_type))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Extracting arguments')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters from the client\n",
    "    parser.add_argument('--stem', type=bool, default=False)\n",
    "    parser.add_argument('--ngram_range', type=int, default=1)\n",
    "    parser.add_argument('--max_df', type=float, default=1.0)\n",
    "    parser.add_argument('--min_df', type=float, default=1.0)\n",
    "    parser.add_argument('--max_features', type=int, default=None)\n",
    "    parser.add_argument('--smooth-idf', type=str, default='true')\n",
    "    parser.add_argument('--sublinear-tf', type=str, default='true')\n",
    "    parser.add_argument('--C', type=float, default=1.0)\n",
    "    parser.add_argument('--penalty', type=str, default='l2')\n",
    "    parser.add_argument('--loss', type=str, default='squared_hinge')\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    # TODO Remove this argument?\n",
    "    # parser.add_argument('--output-data-dir', type=str,\n",
    "    #                     default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model-dir', type=str,\n",
    "                        default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str,\n",
    "                        default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str,\n",
    "                        default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='twitter_train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='twitter_test.csv')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print('Reading Tweets')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    print('Length of train_df: {}'.format(str(len(train_df.index))))\n",
    "    print('Length of test_df: {}'.format(str(len(test_df.index))))\n",
    "\n",
    "    print('Preprocessing the Tweets')\n",
    "    # Decode Sentiment/Target\n",
    "    train_df.target = train_df.target.apply(lambda x: decode_sentiment(x))\n",
    "    test_df.target = test_df.target.apply(lambda x: decode_sentiment(x))\n",
    "\n",
    "    # Preprocess Tweet\n",
    "    train_df.tweet = train_df.tweet.apply(lambda x: preprocess(x, args.stem))\n",
    "    test_df.tweet = test_df.tweet.apply(lambda x: preprocess(x, args.stem))\n",
    "\n",
    "    print('Building training and testing datasets')\n",
    "    X_train = train_df['tweet']\n",
    "    y_train = train_df['target']\n",
    "    X_test = test_df['tweet']\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    # TODO TfIdf preprocessor?\n",
    "\n",
    "    print('Training the model')\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(ngram_range=(1, args.ngram_range),\n",
    "                # max_df=args.max_df,\n",
    "                # min_df=args.min_df,\n",
    "                # max_features=args.max_features,\n",
    "                smooth_idf=bool(strtobool(args.smooth_idf)),\n",
    "                sublinear_tf=bool(strtobool(args.sublinear_tf)))),\n",
    "        # We can't use LinearSVC with a soft VotingClassifier\n",
    "        # A hard VotingCLassifier doesn't fit my needs. We need probabilities, so that we can rank. \n",
    "        ('svc', LinearSVC(C=args.C,\n",
    "                penalty=args.penalty,\n",
    "                loss=args.loss))\n",
    "    ])\n",
    "\n",
    "    clf = pipe.fit(X_train, y_train)\n",
    "\n",
    "    print('Print validation statistics')\n",
    "    pred = clf.predict(X_test)\n",
    "    # pred_prob = clf.predict_proba(X_test)\n",
    "    decision = clf.decision_function(X_test)\n",
    "\n",
    "    print(pred)\n",
    "    print(decision)\n",
    "\n",
    "    # TODO Why the fuck are these the same?\n",
    "    print('Accuracy: {}'.format(accuracy_score(y_test, pred)))\n",
    "    # print('Precision: {}'.format(precision_score(y_test, pred, average='macro')))\n",
    "    # print('Recall: {}'.format(recall_score(y_test, pred, average='micro')))\n",
    "    # TODO Add more validation statistics\n",
    "\n",
    "    print('Save the model')\n",
    "    joblib.dump(clf, os.path.join(args.model_dir, 'model.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SageMaker Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='script.py',\n",
    "    source_dir='.',\n",
    "    role = 'arn:aws:iam::159307201141:role/service-role/AmazonSageMaker-ExecutionRole-20190313T204620',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.large',\n",
    "    framework_version='0.20.0',\n",
    "    base_job_name='twitter-svc',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'accuracy',\n",
    "         'Regex': 'Accuracy: ([0-9.]+).*$'}],\n",
    "    hyperparameters={\n",
    "        'ngram_range': 3,\n",
    "        'C': 100,\n",
    "        'smooth_idf': 'true',\n",
    "        'sublinear_tf': 'false',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2020-03-26 19:47:47 Starting - Starting the training job...\n2020-03-26 19:47:49 Starting - Launching requested ML instances.........\n2020-03-26 19:49:49 Starting - Preparing the instances for training......\n2020-03-26 19:50:35 Downloading - Downloading input data...\n2020-03-26 19:51:26 Training - Training image download completed. Training in progress..\u001b[34m2020-03-26 19:51:26,756 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n\u001b[34m2020-03-26 19:51:26,758 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n\u001b[34m2020-03-26 19:51:26,767 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n\u001b[34m2020-03-26 19:51:44,465 sagemaker-containers INFO     Module script does not provide a setup.py. \u001b[0m\n\u001b[34mGenerating setup.py\u001b[0m\n\u001b[34m2020-03-26 19:51:44,465 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n\u001b[34m2020-03-26 19:51:44,465 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n\u001b[34m2020-03-26 19:51:44,465 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n\u001b[34m/miniconda3/bin/python -m pip install . -r requirements.txt\u001b[0m\n\u001b[34mProcessing /opt/ml/code\u001b[0m\n\u001b[34mCollecting nltk\n  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\u001b[0m\n\u001b[34mRequirement already satisfied: six in /miniconda3/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n\u001b[34mBuilding wheels for collected packages: nltk, script\n  Building wheel for nltk (setup.py): started\u001b[0m\n\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449909 sha256=6a856732c10076272fe024d3a7c4522802addd2747442b0827cacf2260baca20\n  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n  Building wheel for script (setup.py): started\u001b[0m\n\u001b[34m  Building wheel for script (setup.py): finished with status 'done'\n  Created wheel for script: filename=script-1.0.0-py2.py3-none-any.whl size=151657492 sha256=80f8d3e5b055c23ce72af341f88b2563a86f67d902fcf730e44e7cf2a67760de\n  Stored in directory: /tmp/pip-ephem-wheel-cache-2ojtz20c/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n\u001b[34mSuccessfully built nltk script\u001b[0m\n\u001b[34mInstalling collected packages: nltk, script\u001b[0m\n\u001b[34mSuccessfully installed nltk-3.4.5 script-1.0.0\u001b[0m\n\u001b[34m2020-03-26 19:52:19,411 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n\u001b[34m2020-03-26 19:52:19,421 sagemaker-containers INFO     Invoking user script\n\u001b[0m\n\u001b[34mTraining Env:\n\u001b[0m\n\u001b[34m{\n    \"additional_framework_parameters\": {},\n    \"channel_input_dirs\": {\n        \"test\": \"/opt/ml/input/data/test\",\n        \"train\": \"/opt/ml/input/data/train\"\n    },\n    \"current_host\": \"algo-1\",\n    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n    \"hosts\": [\n        \"algo-1\"\n    ],\n    \"hyperparameters\": {\n        \"sublinear_tf\": \"false\",\n        \"C\": 100,\n        \"smooth_idf\": \"true\",\n        \"ngram_range\": 3\n    },\n    \"input_config_dir\": \"/opt/ml/input/config\",\n    \"input_data_config\": {\n        \"test\": {\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        },\n        \"train\": {\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        }\n    },\n    \"input_dir\": \"/opt/ml/input\",\n    \"is_master\": true,\n    \"job_name\": \"twitter-svc-2020-03-26-19-41-36-941\",\n    \"log_level\": 20,\n    \"master_hostname\": \"algo-1\",\n    \"model_dir\": \"/opt/ml/model\",\n    \"module_dir\": \"s3://sagemaker-us-east-1-159307201141/twitter-svc-2020-03-26-19-41-36-941/source/sourcedir.tar.gz\",\n    \"module_name\": \"script\",\n    \"network_interface_name\": \"eth0\",\n    \"num_cpus\": 2,\n    \"num_gpus\": 0,\n    \"output_data_dir\": \"/opt/ml/output/data\",\n    \"output_dir\": \"/opt/ml/output\",\n    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n    \"resource_config\": {\n        \"current_host\": \"algo-1\",\n        \"hosts\": [\n            \"algo-1\"\n        ],\n        \"network_interface_name\": \"eth0\"\n    },\n    \"user_entry_point\": \"script.py\"\u001b[0m\n\u001b[34m}\n\u001b[0m\n\u001b[34mEnvironment variables:\n\u001b[0m\n\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n\u001b[34mSM_HPS={\"C\":100,\"ngram_range\":3,\"smooth_idf\":\"true\",\"sublinear_tf\":\"false\"}\u001b[0m\n\u001b[34mSM_USER_ENTRY_POINT=script.py\u001b[0m\n\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n\u001b[34mSM_MODULE_NAME=script\u001b[0m\n\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n\u001b[34mSM_NUM_CPUS=2\u001b[0m\n\u001b[34mSM_NUM_GPUS=0\u001b[0m\n\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-159307201141/twitter-svc-2020-03-26-19-41-36-941/source/sourcedir.tar.gz\u001b[0m\n\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"C\":100,\"ngram_range\":3,\"smooth_idf\":\"true\",\"sublinear_tf\":\"false\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"twitter-svc-2020-03-26-19-41-36-941\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-159307201141/twitter-svc-2020-03-26-19-41-36-941/source/sourcedir.tar.gz\",\"module_name\":\"script\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"script.py\"}\u001b[0m\n\u001b[34mSM_USER_ARGS=[\"-C\",\"100\",\"--ngram_range\",\"3\",\"--smooth_idf\",\"true\",\"--sublinear_tf\",\"false\"]\u001b[0m\n\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n\u001b[34mSM_HP_SUBLINEAR_TF=false\u001b[0m\n\u001b[34mSM_HP_C=100\u001b[0m\n\u001b[34mSM_HP_SMOOTH_IDF=true\u001b[0m\n\u001b[34mSM_HP_NGRAM_RANGE=3\u001b[0m\n\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n\u001b[0m\n\u001b[34mInvoking script with the following command:\n\u001b[0m\n\u001b[34m/miniconda3/bin/python -m script -C 100 --ngram_range 3 --smooth_idf true --sublinear_tf false\n\n\u001b[0m\n\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  import imp\u001b[0m\n\u001b[34m[nltk_data] Downloading package stopwords to /root/nltk_data...\u001b[0m\n\u001b[34m[nltk_data]   Unzipping corpora/stopwords.zip.\u001b[0m\n\u001b[34mExtracting arguments\u001b[0m\n\u001b[34mReading Tweets\u001b[0m\n\u001b[34mLength of train_df: 1280000\u001b[0m\n\u001b[34mLength of test_df: 320000\u001b[0m\n\u001b[34mPreprocessing the Tweets\u001b[0m\n\u001b[34mBuilding training and testing datasets\u001b[0m\n\u001b[34mTraining the model\u001b[0m\n\u001b[34mPrint validation statistics\u001b[0m\n\u001b[34m['NEGATIVE' 'POSITIVE' 'NEGATIVE' ... 'POSITIVE' 'NEGATIVE' 'POSITIVE']\u001b[0m\n\u001b[34m[-0.63608462  0.55280213 -1.55709413 ...  0.89435417 -2.43366648\n  1.81832297]\u001b[0m\n\u001b[34mAccuracy: 0.790071875\u001b[0m\n\u001b[34mSave the model\u001b[0m\n\n2020-03-26 19:56:47 Uploading - Uploading generated training model\u001b[34m2020-03-26 19:56:44,196 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n\u001b[34mCommand \"/miniconda3/bin/python -m script -C 100 --ngram_range 3 --smooth_idf true --sublinear_tf false\"\u001b[0m\n\n2020-03-26 19:57:45 Failed - Training job failed\n"
    },
    {
     "output_type": "error",
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job twitter-svc-2020-03-26-19-41-36-941: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python -m script -C 100 --ngram_range 3 --smooth_idf true --sublinear_tf false\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9e2ea0f1026a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msklearn_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m's3://sagemaker-us-east-1-159307201141/data/twitter/twitter_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m's3://sagemaker-us-east-1-159307201141/data/twitter/twitter_test.csv'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sagemaker\\estimator.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m   1056\u001b[0m         \u001b[1;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"None\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1058\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1059\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[1;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3020\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3021\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TrainingJobStatus\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[1;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[0;32m   2613\u001b[0m                 ),\n\u001b[0;32m   2614\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Completed\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Stopped\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2615\u001b[1;33m                 \u001b[0mactual_status\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2616\u001b[0m             )\n\u001b[0;32m   2617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m: Error for Training job twitter-svc-2020-03-26-19-41-36-941: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python -m script -C 100 --ngram_range 3 --smooth_idf true --sublinear_tf false\""
     ]
    }
   ],
   "source": [
    "sklearn_estimator.fit({'train':'s3://sagemaker-us-east-1-159307201141/data/twitter/twitter_train.csv', 'test':'s3://sagemaker-us-east-1-159307201141/data/twitter/twitter_test.csv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Launching a tuning job with the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Hyperparameter Tuner \n",
    "\n",
    "from sagemaker.tuner import ContinuousParameter, CategoricalParameter, IntegerParameter\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'ngram_range': IntegerParameter(1, 2),\n",
    "    # 'max_df': ContinuousParameter(0.0001, 1, 'Logarithmic'),\n",
    "    # 'min_df': ContinuousParameter(0.0001, 1, 'Logarithmic'),\n",
    "    # 'max_features': IntegerParameter(1000, 100000),\n",
    "    # 'use_idf': CategoricalParameter(['true', 'false']),\n",
    "    'smooth_idf': CategoricalParameter(['true', 'false']),\n",
    "    'sublinear_tf': CategoricalParameter(['true', 'false']),\n",
    "    'C': ContinuousParameter(0.0001, 100, 'Logarithmic')\n",
    "    # 'penalty': CategoricalParameter(['l1', 'l2']),\n",
    "    # 'loss': CategoricalParameter(['hinge', 'squared_hinge'])\n",
    "}\n",
    "\n",
    "Optimizer = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=sklearn_estimator,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    base_tuning_job_name='twitter-svc-tuner',\n",
    "    objective_type='Maximize',\n",
    "    objective_metric_name='accuracy',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'accuracy',\n",
    "         'Regex': 'Accuracy: ([0-9.]+).*$'}\n",
    "    ],\n",
    "    max_jobs=40,\n",
    "    max_parallel_jobs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer.fit({'train':trainpath, 'test':testpath})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Optimizer' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0639ca0fbcce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalytics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "results = Optimizer.analytics().dataframe()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deploy to a real-time endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2020-03-24 20:21:54 Starting - Preparing the instances for training\n2020-03-24 20:21:54 Downloading - Downloading input data\n2020-03-24 20:21:54 Training - Training image download completed. Training in progress.\n2020-03-24 20:21:54 Uploading - Uploading generated training model\n2020-03-24 20:21:54 Completed - Training job completed\u001b[34m2020-03-24 20:18:22,268 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n\u001b[34m2020-03-24 20:18:22,268 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value accuracy to Json.\u001b[0m\n\u001b[34mReturning the value itself\u001b[0m\n\u001b[34m2020-03-24 20:18:22,271 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n\u001b[34m2020-03-24 20:18:22,280 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n\u001b[34m2020-03-24 20:19:26,376 sagemaker-containers INFO     Module script does not provide a setup.py. \u001b[0m\n\u001b[34mGenerating setup.py\u001b[0m\n\u001b[34m2020-03-24 20:19:26,377 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n\u001b[34m2020-03-24 20:19:26,377 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n\u001b[34m2020-03-24 20:19:26,377 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n\u001b[34m/miniconda3/bin/python -m pip install . -r requirements.txt\u001b[0m\n\u001b[34mProcessing /opt/ml/code\u001b[0m\n\u001b[34mCollecting nltk\n  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\u001b[0m\n\u001b[34mRequirement already satisfied: six in /miniconda3/lib/python3.7/site-packages (from nltk->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n\u001b[34mBuilding wheels for collected packages: nltk, script\n  Building wheel for nltk (setup.py): started\u001b[0m\n\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449908 sha256=b3b78142117d525e119a9f2cc19fef9f57436e82ceef68a584417fbaaee2e49e\n  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n  Building wheel for script (setup.py): started\u001b[0m\n\u001b[34m  Building wheel for script (setup.py): finished with status 'done'\n  Created wheel for script: filename=script-1.0.0-py2.py3-none-any.whl size=151644817 sha256=ee4238ce1f3be912408edbaae91dc3fd22991cfe20ffcc2b8dd9457f91a1e455\n  Stored in directory: /tmp/pip-ephem-wheel-cache-0_ieeot4/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n\u001b[34mSuccessfully built nltk script\u001b[0m\n\u001b[34mInstalling collected packages: nltk, script\u001b[0m\n\u001b[34mSuccessfully installed nltk-3.4.5 script-1.0.0\u001b[0m\n\u001b[34m2020-03-24 20:20:03,969 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value accuracy to Json.\u001b[0m\n\u001b[34mReturning the value itself\u001b[0m\n\u001b[34m2020-03-24 20:20:03,973 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n\u001b[34m2020-03-24 20:20:03,983 sagemaker-containers INFO     Invoking user script\n\u001b[0m\n\u001b[34mTraining Env:\n\u001b[0m\n\u001b[34m{\n    \"additional_framework_parameters\": {\n        \"sagemaker_estimator_module\": \"sagemaker.sklearn.estimator\",\n        \"sagemaker_estimator_class_name\": \"SKLearn\"\n    },\n    \"channel_input_dirs\": {\n        \"test\": \"/opt/ml/input/data/test\",\n        \"train\": \"/opt/ml/input/data/train\"\n    },\n    \"current_host\": \"algo-1\",\n    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n    \"hosts\": [\n        \"algo-1\"\n    ],\n    \"hyperparameters\": {\n        \"C\": 0.0010731517050389413,\n        \"sublinear_tf\": \"false\",\n        \"max_features\": 91147,\n        \"min_df\": 0.0001,\n        \"smooth_idf\": \"true\"\n    },\n    \"input_config_dir\": \"/opt/ml/input/config\",\n    \"input_data_config\": {\n        \"test\": {\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        },\n        \"train\": {\n            \"TrainingInputMode\": \"File\",\n            \"S3DistributionType\": \"FullyReplicated\",\n            \"RecordWrapperType\": \"None\"\n        }\n    },\n    \"input_dir\": \"/opt/ml/input\",\n    \"is_master\": true,\n    \"job_name\": \"twitter-svc-tuner-200324-1449-015-0d842270\",\n    \"log_level\": 20,\n    \"master_hostname\": \"algo-1\",\n    \"model_dir\": \"/opt/ml/model\",\n    \"module_dir\": \"s3://sagemaker-us-east-1-159307201141/twitter-svc-2020-03-24-19-44-12-183/source/sourcedir.tar.gz\",\n    \"module_name\": \"script\",\n    \"network_interface_name\": \"eth0\",\n    \"num_cpus\": 2,\n    \"num_gpus\": 0,\n    \"output_data_dir\": \"/opt/ml/output/data\",\n    \"output_dir\": \"/opt/ml/output\",\n    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n    \"resource_config\": {\n        \"current_host\": \"algo-1\",\n        \"hosts\": [\n            \"algo-1\"\n        ],\n        \"network_interface_name\": \"eth0\"\n    },\n    \"user_entry_point\": \"script.py\"\u001b[0m\n\u001b[34m}\n\u001b[0m\n\u001b[34mEnvironment variables:\n\u001b[0m\n\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n\u001b[34mSM_HPS={\"C\":0.0010731517050389413,\"max_features\":91147,\"min_df\":0.0001,\"smooth_idf\":\"true\",\"sublinear_tf\":\"false\"}\u001b[0m\n\u001b[34mSM_USER_ENTRY_POINT=script.py\u001b[0m\n\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator_class_name\":\"SKLearn\",\"sagemaker_estimator_module\":\"sagemaker.sklearn.estimator\"}\u001b[0m\n\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n\u001b[34mSM_MODULE_NAME=script\u001b[0m\n\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n\u001b[34mSM_NUM_CPUS=2\u001b[0m\n\u001b[34mSM_NUM_GPUS=0\u001b[0m\n\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-159307201141/twitter-svc-2020-03-24-19-44-12-183/source/sourcedir.tar.gz\u001b[0m\n\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator_class_name\":\"SKLearn\",\"sagemaker_estimator_module\":\"sagemaker.sklearn.estimator\"},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"C\":0.0010731517050389413,\"max_features\":91147,\"min_df\":0.0001,\"smooth_idf\":\"true\",\"sublinear_tf\":\"false\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"twitter-svc-tuner-200324-1449-015-0d842270\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-159307201141/twitter-svc-2020-03-24-19-44-12-183/source/sourcedir.tar.gz\",\"module_name\":\"script\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"script.py\"}\u001b[0m\n\u001b[34mSM_USER_ARGS=[\"-C\",\"0.0010731517050389413\",\"--max_features\",\"91147\",\"--min_df\",\"0.0001\",\"--smooth_idf\",\"true\",\"--sublinear_tf\",\"false\"]\u001b[0m\n\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n\u001b[34mSM_HP_C=0.0010731517050389413\u001b[0m\n\u001b[34mSM_HP_SUBLINEAR_TF=false\u001b[0m\n\u001b[34mSM_HP_MAX_FEATURES=91147\u001b[0m\n\u001b[34mSM_HP_MIN_DF=0.0001\u001b[0m\n\u001b[34mSM_HP_SMOOTH_IDF=true\u001b[0m\n\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n\u001b[0m\n\u001b[34mInvoking script with the following command:\n\u001b[0m\n\u001b[34m/miniconda3/bin/python -m script -C 0.0010731517050389413 --max_features 91147 --min_df 0.0001 --smooth_idf true --sublinear_tf false\n\n\u001b[0m\n\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  import imp\u001b[0m\n\u001b[34m[nltk_data] Downloading package stopwords to /root/nltk_data...\u001b[0m\n\u001b[34m[nltk_data]   Unzipping corpora/stopwords.zip.\u001b[0m\n\u001b[34mExtracting arguments\u001b[0m\n\u001b[34mReading Tweets\u001b[0m\n\u001b[34mLength of train_df: 1280000\u001b[0m\n\u001b[34mLength of test_df: 320000\u001b[0m\n\u001b[34mPreprocessing the Tweets\u001b[0m\n\u001b[34mBuilding training and testing datasets\u001b[0m\n\u001b[34mTraining the model\u001b[0m\n\u001b[34mPrint validation statistics\u001b[0m\n\u001b[34m['NEGATIVE' 'POSITIVE' 'NEGATIVE' ... 'POSITIVE' 'NEGATIVE' 'POSITIVE']\u001b[0m\n\u001b[34m[-0.41147226  0.24399938 -0.26128284 ...  0.69216641 -1.77587798\n  0.9347455 ]\u001b[0m\n\u001b[34mAccuracy: 0.77144375\u001b[0m\n\u001b[34mSave the model\u001b[0m\n\u001b[34m2020-03-24 20:21:43,011 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\nTraining seconds: 261\nBillable seconds: 261\n-------------------!"
    }
   ],
   "source": [
    "#TODO Get best model from a tuning job OR do it by name\n",
    "\n",
    "my_training_job_name = \"twitter-svc-tuner-200324-1449-015-0d842270\"\n",
    "sklearn_estimator = SKLearn.attach(my_training_job_name)\n",
    "\n",
    "predictor = sklearn_estimator.deploy(\n",
    "    instance_type='ml.t2.medium',\n",
    "    initial_instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Invoke with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'prediction': 'POSITIVE', 'probability': 0.6611542933909837}\n{'prediction': 'POSITIVE', 'probability': 0.15763187223286762}\n{'prediction': 'POSITIVE', 'probability': 0.013655673248487599}\n{'prediction': 'POSITIVE', 'probability': 0.7376460738221517}\n{'prediction': 'POSITIVE', 'probability': 0.2431036530170101}\n{'prediction': 'NEGATIVE', 'probability': -0.8907029657259222}\n"
    }
   ],
   "source": [
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "tweets = {\n",
    "    'tweet': [\n",
    "        '''Here to give you dinner inspo, so you don't keep eating chips for dinner.''',\n",
    "        '''totally okay to not love Biden but if you’re threatening to just not vote in the election if he’s the candidate, you suck''',\n",
    "        '''Spent 2 days in New Orleans. Gained 75 lbs. Totally worth it.''',\n",
    "        '''Fresh pastry from Beetbox in the house!''',\n",
    "        '''These people made me laugh so hard that I briefly thought I gave myself a hernia.''',\n",
    "        '''I hate everything.'''\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName='twitter-svc-tuner-200324-1449-015-0d842270',\n",
    "    Body=json.dumps(tweets),\n",
    "    ContentType='application/json')\n",
    "\n",
    "results = json.loads(response['Body'].read())\n",
    "\n",
    "for pred in results['results']:\n",
    "    print(pred)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "neutral : 0.0\nnegative : -0.8761\npositive : 0.5849\npositive : 0.3802\npositive : 0.4086\nnegative : -0.5719\n"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in tweets['tweet']:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    # print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "\n",
    "    compound = vs['compound']\n",
    "\n",
    "    if compound >= 0.05:\n",
    "        score = 'positive'\n",
    "    if compound > -0.05 and compound < 0.05:\n",
    "        score = 'neutral'\n",
    "    if compound <= -0.05:\n",
    "        score = 'negative'\n",
    "\n",
    "    print('{} : {}'.format(score, compound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3.delete_endpoint(EndpointName=predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt\n",
    "\n",
    "\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# remove hashtag\n",
    "\n",
    "# lower everything\n",
    "\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing\n",
    "# combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "    \n",
    "combi['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         target         ids                          date      flag  \\\n0        0       1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1        0       1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2        0       1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3        0       1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4        0       1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n...     ..              ...                           ...       ...   \n1599995  4       2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599996  4       2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599997  4       2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599998  4       2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599999  4       2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n\n                    user  \\\n0        _TheSpecialOne_   \n1        scotthamilton     \n2        mattycus          \n3        ElleCTF           \n4        Karoli            \n...         ...            \n1599995  AmandaMarie1028   \n1599996  TheWDBoards       \n1599997  bpbabe            \n1599998  tinydiamondz      \n1599999  RyanTrevMorris    \n\n                                                                                                                       tweet  \n0        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n1        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      \n2        @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            \n3        my whole body feels itchy and like its on fire                                                                       \n4        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       \n...                                                                                                                  ...      \n1599995  Just woke up. Having no school is the best feeling ever                                                              \n1599996  TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta                                       \n1599997  Are you ready for your MoJo Makeover? Ask me for details                                                             \n1599998  Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur                                                     \n1599999  happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H                                                        \n\n[1600000 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>ids</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1599995</th>\n      <td>4</td>\n      <td>2193601966</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>AmandaMarie1028</td>\n      <td>Just woke up. Having no school is the best feeling ever</td>\n    </tr>\n    <tr>\n      <th>1599996</th>\n      <td>4</td>\n      <td>2193601969</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>TheWDBoards</td>\n      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n    </tr>\n    <tr>\n      <th>1599997</th>\n      <td>4</td>\n      <td>2193601991</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>bpbabe</td>\n      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n    </tr>\n    <tr>\n      <th>1599998</th>\n      <td>4</td>\n      <td>2193602064</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>tinydiamondz</td>\n      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n    </tr>\n    <tr>\n      <th>1599999</th>\n      <td>4</td>\n      <td>2193602129</td>\n      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>RyanTrevMorris</td>\n      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600000 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "DATASET_COLUMNS = ['target', 'ids', 'date', 'flag', 'user', 'tweet']\n",
    "DATASET_ENCODING = 'ISO-8859-1'\n",
    "\n",
    "# Read the data locally\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',\n",
    "                 encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         target         ids                          date      flag  \\\n0        0       1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n1        0       1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n2        0       1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n3        0       1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n4        0       1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n...     ..              ...                           ...       ...   \n1599995  4       2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599996  4       2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599997  4       2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599998  4       2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n1599999  4       2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n\n                    user  \\\n0        _TheSpecialOne_   \n1        scotthamilton     \n2        mattycus          \n3        ElleCTF           \n4        Karoli            \n...         ...            \n1599995  AmandaMarie1028   \n1599996  TheWDBoards       \n1599997  bpbabe            \n1599998  tinydiamondz      \n1599999  RyanTrevMorris    \n\n                                                                                                                  tweet  \n0        - Awww, that's a bummer. You shoulda got David Carr of Third Day to do it.                                      \n1        is upset that he can't update his Facebook by texting it... and might cry as a result School today also. Blah!  \n2        I dived many times for the ball. Managed to save 50% The rest go out of bounds                                  \n3        my whole body feels itchy and like its on fire                                                                  \n4        no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.                   \n...                                                                                                ...                   \n1599995  Just woke up. Having no school is the best feeling ever                                                         \n1599996  TheWDB.com - Very cool to hear old Walt interviews! â«                                                         \n1599997  Are you ready for your MoJo Makeover? Ask me for details                                                        \n1599998  Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur                                                \n1599999  happy                                                                                                           \n\n[1600000 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>ids</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>- Awww, that's a bummer. You shoulda got David Carr of Third Day to do it.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by texting it... and might cry as a result School today also. Blah!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>I dived many times for the ball. Managed to save 50% The rest go out of bounds</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1599995</th>\n      <td>4</td>\n      <td>2193601966</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>AmandaMarie1028</td>\n      <td>Just woke up. Having no school is the best feeling ever</td>\n    </tr>\n    <tr>\n      <th>1599996</th>\n      <td>4</td>\n      <td>2193601969</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>TheWDBoards</td>\n      <td>TheWDB.com - Very cool to hear old Walt interviews! â«</td>\n    </tr>\n    <tr>\n      <th>1599997</th>\n      <td>4</td>\n      <td>2193601991</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>bpbabe</td>\n      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n    </tr>\n    <tr>\n      <th>1599998</th>\n      <td>4</td>\n      <td>2193602064</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>tinydiamondz</td>\n      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n    </tr>\n    <tr>\n      <th>1599999</th>\n      <td>4</td>\n      <td>2193602129</td>\n      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>RyanTrevMorris</td>\n      <td>happy</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600000 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "import preprocessor as p \n",
    "\n",
    "p.set_options(p.OPT.HASHTAG)\n",
    "df['tweet'] = df['tweet'].apply(lambda x: p.clean(x))\n",
    "\n",
    "df"
   ]
  }
 ]
}